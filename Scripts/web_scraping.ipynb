{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c7685d-43e7-4398-9adb-cad163264c01",
   "metadata": {},
   "source": [
    "## Scraping info for cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9609be7-41e7-4ef5-a361-6ae456b20944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3def7cfb-8794-4d08-ad95-9e2e048fb09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define a function \n",
    "\n",
    "def wiki_info(cities):\n",
    "    \n",
    "    list_for_df = []\n",
    "    \n",
    "    for city in cities:\n",
    "        \n",
    "        url = url = f'https://en.wikipedia.org/wiki/{city}'\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        \n",
    "        response_dict = {}\n",
    "        \n",
    "        \n",
    "        response_dict['city'] = soup.select('#firstHeading')[0].get_text()\n",
    "        response_dict['country'] = soup.select(\".infobox-data\")[0].get_text()\n",
    "        response_dict['latitude'] = soup.select(\".latitude\")[0].get_text()\n",
    "        response_dict['longitude'] = soup.select(\".longitude\")[0].get_text()\n",
    "        \n",
    "        if soup.select_one('.infobox-label:-soup-contains(\"Elevation\")'):\n",
    "            response_dict['elevation'] = soup.select_one('.infobox-label:-soup-contains(\"Elevation\")').find_next(class_='infobox-data').get_text()\n",
    "            response_dict['website'] = soup.select_one('.infobox-label:-soup-contains(\"Website\")').find_next(class_='infobox-data').get_text()\n",
    "        \n",
    "        if soup.select_one('th.infobox-header:-soup-contains(\"Population\")'):\n",
    "            response_dict['population'] = soup.select_one('th.infobox-header:-soup-contains(\"Population\")').parent.find_next_sibling().find(text=re.compile(r'\\d+'))\n",
    "\n",
    "        \n",
    "        list_for_df.append(response_dict)\n",
    "        \n",
    "    # creating the DataFrame\n",
    "    cities_info = pd.DataFrame(list_for_df)\n",
    "    \n",
    "    \n",
    "    return cities_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd4e38-279c-466e-a562-133dd1fce910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial with 3 cities\n",
    "cities = ['Berlin', 'Hamburg', 'London']\n",
    "wiki_info(cities)\n",
    "\n",
    "# the list of the cities could be up to anyone\n",
    "# one could generate a list with an AI generator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
